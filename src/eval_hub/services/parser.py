"""Request parser service for evaluation specifications."""

from typing import Dict, List, Any, Optional
from uuid import UUID

from ..core.config import Settings
from ..core.exceptions import ValidationError
from ..core.logging import get_logger
from ..models.evaluation import (
    EvaluationRequest,
    EvaluationSpec,
    BackendSpec,
    BenchmarkSpec,
    RiskCategory,
    BackendType,
)


class RequestParser:
    """Parser for evaluation requests and specifications."""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.logger = get_logger(__name__)

    async def parse_evaluation_request(self, request: EvaluationRequest) -> EvaluationRequest:
        """Parse and validate an evaluation request."""
        self.logger.info(
            "Parsing evaluation request",
            request_id=str(request.request_id),
            evaluation_count=len(request.evaluations)
        )

        # Validate the request
        await self._validate_request(request)

        # Process each evaluation spec
        processed_evaluations = []
        for eval_spec in request.evaluations:
            processed_spec = await self._process_evaluation_spec(eval_spec)
            processed_evaluations.append(processed_spec)

        # Create processed request
        processed_request = request.model_copy()
        processed_request.evaluations = processed_evaluations

        self.logger.info(
            "Successfully parsed evaluation request",
            request_id=str(request.request_id),
            total_backends=sum(len(spec.backends) for spec in processed_evaluations)
        )

        return processed_request

    async def _validate_request(self, request: EvaluationRequest) -> None:
        """Validate the evaluation request."""
        if not request.evaluations:
            raise ValidationError("Request must contain at least one evaluation")

        if len(request.evaluations) > 100:
            raise ValidationError("Request cannot contain more than 100 evaluations")

        # Validate each evaluation
        for i, eval_spec in enumerate(request.evaluations):
            await self._validate_evaluation_spec(eval_spec, f"evaluations[{i}]")

    async def _validate_evaluation_spec(self, spec: EvaluationSpec, context: str) -> None:
        """Validate a single evaluation specification."""
        if not spec.model_name:
            raise ValidationError(f"{context}: model_name is required")

        if not spec.backends and not spec.risk_category:
            raise ValidationError(f"{context}: must specify either backends or risk_category")

        if spec.timeout_minutes <= 0:
            raise ValidationError(f"{context}: timeout_minutes must be positive")

        if spec.retry_attempts < 0:
            raise ValidationError(f"{context}: retry_attempts cannot be negative")

        # Validate backends if provided
        if spec.backends:
            for j, backend in enumerate(spec.backends):
                await self._validate_backend_spec(backend, f"{context}.backends[{j}]")

    async def _validate_backend_spec(self, backend: BackendSpec, context: str) -> None:
        """Validate a backend specification."""
        if not backend.name:
            raise ValidationError(f"{context}: name is required")

        if not backend.benchmarks:
            raise ValidationError(f"{context}: must specify at least one benchmark")

        # Check if backend type is supported
        supported_backends = self.settings.backend_configs.keys()
        if backend.type not in [BackendType.CUSTOM] and backend.name not in supported_backends:
            raise ValidationError(
                f"{context}: unsupported backend '{backend.name}'. "
                f"Supported backends: {list(supported_backends)}"
            )

        # Validate benchmarks
        for k, benchmark in enumerate(backend.benchmarks):
            await self._validate_benchmark_spec(benchmark, f"{context}.benchmarks[{k}]")

    async def _validate_benchmark_spec(self, benchmark: BenchmarkSpec, context: str) -> None:
        """Validate a benchmark specification."""
        if not benchmark.name:
            raise ValidationError(f"{context}: name is required")

        if not benchmark.tasks:
            raise ValidationError(f"{context}: must specify at least one task")

        if benchmark.num_fewshot is not None and benchmark.num_fewshot < 0:
            raise ValidationError(f"{context}: num_fewshot cannot be negative")

        if benchmark.batch_size is not None and benchmark.batch_size <= 0:
            raise ValidationError(f"{context}: batch_size must be positive")

        if benchmark.limit is not None and benchmark.limit <= 0:
            raise ValidationError(f"{context}: limit must be positive")

    async def _process_evaluation_spec(self, spec: EvaluationSpec) -> EvaluationSpec:
        """Process an evaluation specification, expanding risk categories if needed."""
        processed_spec = spec.model_copy()

        # If risk category is specified but no backends, generate backends
        if spec.risk_category and not spec.backends:
            self.logger.info(
                "Generating backends from risk category",
                evaluation_id=str(spec.id),
                risk_category=spec.risk_category
            )
            processed_spec.backends = await self._generate_backends_from_risk_category(
                spec.risk_category, spec.model_name
            )

        # Apply default configurations to backends
        for backend in processed_spec.backends:
            await self._apply_backend_defaults(backend)

        return processed_spec

    async def _generate_backends_from_risk_category(
        self, risk_category: RiskCategory, model_name: str
    ) -> List[BackendSpec]:
        """Generate backend specifications based on risk category."""
        risk_config = self.settings.risk_category_benchmarks.get(risk_category.value)
        if not risk_config:
            raise ValidationError(f"No configuration found for risk category: {risk_category}")

        backends = []

        # Generate backend for each configured backend type
        for backend_name, backend_config in self.settings.backend_configs.items():
            # Create benchmarks based on risk category
            benchmarks = []
            for benchmark_name in risk_config["benchmarks"]:
                benchmark = BenchmarkSpec(
                    name=benchmark_name,
                    tasks=[benchmark_name],  # Simplified - each benchmark is a single task
                    num_fewshot=risk_config.get("num_fewshot"),
                    limit=risk_config.get("limit"),
                    config={}
                )
                benchmarks.append(benchmark)

            # Create backend spec
            backend = BackendSpec(
                name=backend_name,
                type=BackendType.LMEVAL if "lm-evaluation" in backend_name else BackendType.GUIDELLM,
                benchmarks=benchmarks,
                config=backend_config.copy()
            )
            backends.append(backend)

            self.logger.debug(
                "Generated backend from risk category",
                backend_name=backend_name,
                risk_category=risk_category,
                benchmark_count=len(benchmarks)
            )

        return backends

    async def _apply_backend_defaults(self, backend: BackendSpec) -> None:
        """Apply default configurations to a backend specification."""
        # Get default config for this backend type
        default_config = self.settings.backend_configs.get(backend.name, {})

        # Merge configurations (backend.config takes precedence)
        merged_config = default_config.copy()
        merged_config.update(backend.config)
        backend.config = merged_config

        # Apply defaults to benchmarks
        for benchmark in backend.benchmarks:
            await self._apply_benchmark_defaults(benchmark, backend.name)

    async def _apply_benchmark_defaults(self, benchmark: BenchmarkSpec, backend_name: str) -> None:
        """Apply default configurations to a benchmark specification."""
        # Set default batch size if not specified
        if benchmark.batch_size is None:
            benchmark.batch_size = 1

        # Set default device if not specified
        if benchmark.device is None:
            benchmark.device = "auto"

        # Apply backend-specific defaults
        if backend_name == "lm-evaluation-harness":
            if benchmark.num_fewshot is None:
                benchmark.num_fewshot = 5

    def get_total_benchmark_count(self, request: EvaluationRequest) -> int:
        """Get the total number of benchmarks across all evaluations."""
        total = 0
        for evaluation in request.evaluations:
            for backend in evaluation.backends:
                total += len(backend.benchmarks)
        return total

    def estimate_completion_time(self, request: EvaluationRequest) -> int:
        """Estimate completion time in minutes for the request."""
        total_benchmarks = self.get_total_benchmark_count(request)

        # Simple estimation: 5 minutes per benchmark on average
        base_time = total_benchmarks * 5

        # Add overhead for concurrent execution
        max_concurrent = self.settings.max_concurrent_evaluations
        if total_benchmarks > max_concurrent:
            # Account for queuing time
            base_time = int(base_time * 1.5)

        return max(base_time, 10)  # Minimum 10 minutes